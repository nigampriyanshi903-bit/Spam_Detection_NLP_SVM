{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a6214c-7f04-41a6-a97e-752553b60162",
   "metadata": {},
   "source": [
    "## Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d951b51-ca5a-420f-983e-3ec80a009cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Data Loading ---\n",
      " Data file successfully loaded.\n",
      "\n",
      "--- Initial Inspection (Head) ---\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "\n",
      "--- Label Distribution ---\n",
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "file_path = r\"C:\\Users\\Asus\\Downloads\\spam.csv\"\n",
    "\n",
    "print(\"--- Step 1: Data Loading ---\")\n",
    "try:\n",
    "\n",
    "    # Note: 'encoding' might be required for some datasets\n",
    "    df = pd.read_csv(file_path, encoding='latin-1') \n",
    "    \n",
    "    # Drop unnecessary columns if they exist (common in Kaggle's SMS dataset)\n",
    "    if 'Unnamed: 2' in df.columns:\n",
    "        df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
    "        \n",
    "    # Rename columns for clarity (v1 -> label, v2 -> message)\n",
    "    df.columns = ['label', 'message']\n",
    "\n",
    "    print(f\" Data file successfully loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\" Error: File not found at '{file_path}'. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "# 2. Initial Inspection\n",
    "print(\"\\n--- Initial Inspection (Head) ---\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n--- Label Distribution ---\")\n",
    "# Check the distribution of 'spam' vs 'ham'\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4b5fd-2751-4199-b830-32a6a6d10a33",
   "metadata": {},
   "source": [
    "## NLP Preprocessing (Text Cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a4c82a0-0ae1-4919-9d0e-200aad879401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Cleaning and Stemming Complete.\n",
      "\n",
      "--- Encoded and Cleaned Data Head ---\n",
      "   label                                            message  \\\n",
      "0      0  Go until jurong point, crazy.. Available only ...   \n",
      "1      0                      Ok lar... Joking wif u oni...   \n",
      "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3      0  U dun say so early hor... U c already then say...   \n",
      "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                     cleaned_message  \n",
      "0  go jurong point crazi avail bugi n great world...  \n",
      "1                              ok lar joke wif u oni  \n",
      "2  free entri wkli comp win fa cup final tkt st m...  \n",
      "3                u dun say earli hor u c alreadi say  \n",
      "4               nah think goe usf live around though  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer # Hum Stemming ka upyog karenge\n",
    "\n",
    "# Download stopwords list if not already downloaded\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "# Initialize stemmer and stopwords\n",
    "ps = PorterStemmer()\n",
    "all_stopwords = stopwords.words('english')\n",
    "\n",
    "# Remove common but non-informative words like 'will' if they interfere\n",
    "# all_stopwords.remove('will') \n",
    "\n",
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    # 1. Remove all non-word characters and numbers (only keep letters)\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # 2. Convert to Lowercase\n",
    "    text = text.lower()\n",
    "    # 3. Split into words\n",
    "    text = text.split()\n",
    "    \n",
    "    # 4. Apply Stemming and remove Stopwords\n",
    "    text = [ps.stem(word) for word in text if not word in set(all_stopwords)]\n",
    "    \n",
    "    # 5. Join the words back into a single string\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'message' column\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "\n",
    "print(\"Text Cleaning and Stemming Complete.\")\n",
    "\n",
    "# 2. Label Encoding (Ham=0, Spam=1)\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "print(\"\\n--- Encoded and Cleaned Data Head ---\")\n",
    "print(df[['label', 'message', 'cleaned_message']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0baf1a-8fb9-4439-a28c-c80ab67ffa85",
   "metadata": {},
   "source": [
    "## Text Vectorization (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba3165-12a9-426d-ad86-0788659b111c",
   "metadata": {},
   "source": [
    "#### 1. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cde68a7-4d9e-4b66-b0dd-c08bcf690729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data successfully split.\n",
      "Training set size: 4457 samples\n",
      "Testing set size: 1115 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['cleaned_message']\n",
    "y = df['label']\n",
    "\n",
    "# 80/20 train-test split (X_train: cleaned messages for training, y_train: labels for training)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y # Stratify ensures the 6.46:1 ham/spam ratio is maintained.\n",
    ")\n",
    "\n",
    "print(\" Data successfully split.\")\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cfda94-98d8-4831-8b53-f5ca40c37450",
   "metadata": {},
   "source": [
    "#### 2. Applying TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0fb304-387c-45d5-bd33-0d1e5ad04760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TF-IDF Vectorization Complete.\n",
      "Shape of Training Data (Samples, Features): (4457, 3000)\n",
      "Shape of Testing Data (Samples, Features): (1115, 3000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "# max_features=3000 ka matlab hai ki hum sirf top 3000 sabse zyada important words ko lenge.\n",
    "# Isse model ki complexity kam hoti hai.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "\n",
    "# Fit the vectorizer on the training text data (X_train)\n",
    "# Vectorizer training data mein unique words ki vocabulary, unki TF aur IDF values seekhta hai.\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "# Transform the test text data (X_test) using the fitted vectorizer\n",
    "# Yahaan sirf transform kiya jaata hai, fit nahi.\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "print(\"\\n TF-IDF Vectorization Complete.\")\n",
    "print(f\"Shape of Training Data (Samples, Features): {X_train_tfidf.shape}\")\n",
    "print(f\"Shape of Testing Data (Samples, Features): {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd243858-b3d7-4a75-9899-d530f7bcb978",
   "metadata": {},
   "source": [
    "## Model Training (Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf53eab-c9a7-48e2-a2a3-0d3fd7264451",
   "metadata": {},
   "source": [
    "#### 1. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce95d74c-c0ff-460b-ab00-8896dc91f05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Multinomial Naive Bayes Model...\n",
      " Naive Bayes Model successfully trained.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes model\n",
    "mnb_model = MultinomialNB()\n",
    "\n",
    "# Train the model using the TF-IDF transformed training data\n",
    "print(\"Training Multinomial Naive Bayes Model...\")\n",
    "mnb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\" Naive Bayes Model successfully trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332dfb36-e032-4878-b5c1-1cd447d0a863",
   "metadata": {},
   "source": [
    "#### 2. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125cb84f-a4c3-40d5-9e0c-b6af80574056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Predictions completed.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred_mnb = mnb_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"\\n Predictions completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db59830-d6df-413a-b15f-42454d19960b",
   "metadata": {},
   "source": [
    "#### Evaluation and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd4d7402-5607-4c99-bc0a-3ee7628ced32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy (Test Set): 0.9740\n",
      "\n",
      "--- Confusion Matrix ---\n",
      "[[965   1]\n",
      " [ 28 121]]\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       966\n",
      "           1       0.99      0.81      0.89       149\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.91      0.94      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracy\n",
    "accuracy_mnb = accuracy_score(y_test, y_pred_mnb)\n",
    "print(f\"\\nOverall Accuracy (Test Set): {accuracy_mnb:.4f}\")\n",
    "\n",
    "# Calculate the Confusion Matrix\n",
    "cm_mnb = confusion_matrix(y_test, y_pred_mnb)\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "print(cm_mnb)\n",
    "\n",
    "# Generate the detailed Classification Report\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2a6dff-dec9-4a6e-8647-d99d369351ac",
   "metadata": {},
   "source": [
    "## Model Training (Support Vector Machine - SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf782146-b917-436c-bdc3-ee944ff5ff7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Support Vector Machine (SVM) Model...\n",
      " SVM Model successfully trained.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the Support Vector Machine (SVC) model\n",
    "# Hum linear kernel ka use karenge kyunki data high-dimensional hai (3000 features).\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "# Train the model on the TF-IDF transformed training data\n",
    "print(\"\\nTraining Support Vector Machine (SVM) Model...\")\n",
    "# SVM Training is generally slower than Naive Bayes\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\" SVM Model successfully trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0602fd91-4dc6-4f1d-b42c-de1e325ceb68",
   "metadata": {},
   "source": [
    "## Prediction & Evaluation (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11668e5d-1acb-43e4-bafc-dcfda9a1b0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SVM Predictions completed.\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set using the SVM model\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\" SVM Predictions completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e94bdbb-fe57-468c-82b7-8f07d1018f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Accuracy (SVM Test Set): 0.9857\n",
      "\n",
      "--- SVM Confusion Matrix ---\n",
      "[[965   1]\n",
      " [ 15 134]]\n",
      "\n",
      "--- SVM Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.99      0.90      0.94       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.99      0.95      0.97      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#. Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"\\nOverall Accuracy (SVM Test Set): {accuracy_svm:.4f}\")\n",
    "\n",
    "# Calculate the Confusion Matrix\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "print(\"\\n--- SVM Confusion Matrix ---\")\n",
    "print(cm_svm)\n",
    "\n",
    "# Generate the detailed Classification Report\n",
    "print(\"\\n--- SVM Classification Report ---\")\n",
    "print(classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d0cd9-92c6-4e7a-aa88-41f360d69711",
   "metadata": {},
   "source": [
    "##  Project Conclusion: Best Model Selection (SVM)\n",
    "\n",
    "The Spam Detection project successfully utilized NLP techniques (Cleaning, Stemming, and TF-IDF Vectorization) with two powerful classification algorithms: Multinomial Naive Bayes (MNB) and Support Vector Machine (SVM).\n",
    "\n",
    "### Final Model Comparison\n",
    "\n",
    "| Metric | MNB Score | SVM Score | Final Decision |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Overall Accuracy** | $0.9740$ | **$0.9857$** | SVM (Higher) |\n",
    "| **Precision (Spam)** | $0.99$ | $0.99$ | Excellent for avoiding False Positives. |\n",
    "| **Recall (Spam)** | $0.81$ | **$0.90$** | **SVM is superior.** |\n",
    "| **F1-Score (Spam)** | $0.89$ | **$0.94$** | SVM (Better balance). |\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "1.  **Best Performer: Support Vector Machine (SVM).** SVM delivered a remarkable **$98.57\\%$ accuracy** and, more importantly, achieved **$90\\%$ Recall** for the Spam class.\n",
    "2.  **User Experience Focus:** SVM is highly reliable for a practical spam filter:\n",
    "    * It maintained near-perfect **Precision ($0.99$)** (only 1 legitimate message was misclassified as spam).\n",
    "    * It significantly reduced **False Negatives** (spam messages leaking into the inbox) from 28 (MNB) to just **15** (SVM).\n",
    "\n",
    "The SVM model, using a **linear kernel** on the **TF-IDF vector space**, provides the optimal balance of speed, accuracy, and performance on the crucial positive class (Spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc47254-ac5d-41aa-9f97-3d3b6074fc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c4529-1998-4f25-810b-1534536f8ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a43cfb-d5da-4cb4-803b-749257a1136b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116de66-5fc4-47fb-9a74-e7994d479205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b95f74-d031-4fe4-93ba-64163a932fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94692184-4cd4-4c53-940a-588f78f33304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39257ef-2c4c-4246-be36-6bd32fa36573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03463259-9475-47cf-ac69-c6ec9e83c9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
